\begin{thebibliography}{}

\bibitem[Baly et~al. 2020]{baly2020we}
Baly, R., Da~San~Martino, G., Glass, J., and Nakov, P. (2020).
\newblock We can detect your bias: Predicting the political ideology of news articles.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 4982--4991.

\bibitem[Brink et~al. 2016]{brink2016real}
Brink, H., Richards, J., and Fetherolf, M. (2016).
\newblock Real-world machine learning. manning publications.

\bibitem[Chen et~al. 2018]{chen2018learning}
Chen, W.-F., Wachsmuth, H., Al~Khatib, K., and Stein, B. (2018).
\newblock Learning to flip the bias of news headlines.
\newblock In {\em Proceedings of the 11th International conference on natural language generation}, pages 79--88.

\bibitem[Devlin et~al. 2018]{bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018).
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}.

\bibitem[Goodfellow 2016]{goodfellow2016deep}
Goodfellow, I. (2016).
\newblock Deep learning.

\bibitem[Kertész 2021]{loss}
Kertész, G. (2021).
\newblock Different triplet sampling techniques for lossless triplet loss on metric similarity learning.
\newblock In {\em 2021 IEEE 19th World Symposium on Applied Machine Intelligence and Informatics (SAMI)}, pages 000449--000454.

\bibitem[Lin et~al. 2025]{lin-etal-2025-investigating}
Lin, L., Wang, L., Guo, J., and Wong, K.-F. (2025).
\newblock Investigating bias in {LLM}-based bias detection: Disparities between {LLM}s and human perception.
\newblock In Rambow, O., Wanner, L., Apidianaki, M., Al-Khalifa, H., Eugenio, B.~D., and Schockaert, S., editors, {\em Proceedings of the 31st International Conference on Computational Linguistics}, pages 10634--10649, Abu Dhabi, UAE. Association for Computational Linguistics.

\bibitem[Lin et~al. 2024]{lin2024indivecexplorationleveraginglarge}
Lin, L., Wang, L., Zhao, X., Li, J., and Wong, K.-F. (2024).
\newblock Indivec: An exploration of leveraging large language models for media bias detection with fine-grained bias indicators.

\bibitem[Liu 2019]{roberta}
Liu, Y. (2019).
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}.

\bibitem[Sanh 2019]{distilbert}
Sanh, V. (2019).
\newblock Distilbert, a distilled version of bert: Smaller, faster, cheaper and lighter.
\newblock {\em arXiv preprint arXiv:1910.01108}.

\bibitem[Zhang and Rao 2020]{Zhang}
Zhang, Y. and Rao, Z. (2020).
\newblock Deep neural networks with pre-train model bert for aspect-level sentiments classification.
\newblock In {\em 2020 IEEE 5th Information Technology and Mechatronics Engineering Conference (ITOEC)}, pages 923--927. IEEE.

\end{thebibliography}
